use std::io::{ BufReader, Read };
use lazy_static::lazy_static;
use regex::Regex;
use parse_mediawiki_dump::Page;

lazy_static! {
    static ref WORD_REGEX: Regex =
        Regex::new(r"[A-Za-z][A-Za-z\-]+[A-Za-z]").unwrap();

    // Generated by https://github.com/portstrom/fetch_mediawiki_configuration
    static ref WIKITEXT_CONFIGURATION: parse_wiki_text::Configuration =    
        parse_wiki_text::Configuration::new(&parse_wiki_text::ConfigurationSource {
            category_namespaces: &[
                "category",
            ],
            extension_tags: &[
                "categorytree",
                "ce",
                "charinsert",
                "chem",
                "gallery",
                "graph",
                "hiero",
                "imagemap",
                "indicator",
                "inputbox",
                "mapframe",
                "maplink",
                "math",
                "nowiki",
                "poem",
                "pre",
                "ref",
                "references",
                "score",
                "section",
                "source",
                "syntaxhighlight",
                "templatedata",
                "templatestyles",
                "timeline",
            ],
            file_namespaces: &[
                "file",
                "image",
            ],
            link_trail: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz",
            magic_words: &[
                "DISAMBIG",
                "EXPECTUNUSEDCATEGORY",
                "FORCETOC",
                "HIDDENCAT",
                "INDEX",
                "NEWSECTIONLINK",
                "NOCC",
                "NOCOLLABORATIONHUBTOC",
                "NOCONTENTCONVERT",
                "NOEDITSECTION",
                "NOGALLERY",
                "NOGLOBAL",
                "NOINDEX",
                "NONEWSECTIONLINK",
                "NOTC",
                "NOTITLECONVERT",
                "NOTOC",
                "STATICREDIRECT",
                "TOC",
            ],
            protocols: &[
                "//",
                "bitcoin:",
                "ftp://",
                "ftps://",
                "geo:",
                "git://",
                "gopher://",
                "http://",
                "https://",
                "irc://",
                "ircs://",
                "magnet:",
                "mailto:",
                "mms://",
                "news:",
                "nntp://",
                "redis://",
                "sftp://",
                "sip:",
                "sips:",
                "sms:",
                "ssh://",
                "svn://",
                "tel:",
                "telnet://",
                "urn:",
                "worldwind://",
                "xmpp:",
            ],
            redirect_magic_words: &[
                "REDIRECT",
            ]
        });
}

const WIKITEXT_NAMESPACE: u32 = 0;
const WIKITEXT_MIME_TYPE: &str = "text/x-wiki";
const WIKITEXT_MODEL: &str = "wikitext";

pub fn wikitext_in_mediawiki_dump(reader: impl Read) -> impl Iterator<Item=String> {
    let buffered_reader = BufReader::new(reader);

    parse_mediawiki_dump::parse(buffered_reader)
        .filter_map(|result| {
            if let Ok(Page { namespace, format: Some(format), model: Some(model), text, .. }) = result {
                if  namespace   == WIKITEXT_NAMESPACE &&
                    format      == WIKITEXT_MIME_TYPE &&
                    model       == WIKITEXT_MODEL
                {
                    return Some(text);
                }
            }

            None
        })
}

static TEMPLATE_OPEN_TOKEN:     &str = "{{";
static TEMPLATE_CLOSE_TOKEN:    &str = "}}";

pub fn words_in_wikitext(wikitext: &str) -> Vec<&str> {
    use parse_wiki_text::{
        Node,
        Node::*,
    };

    fn collect_words_in_text<'a>(accumulator: &mut Vec<&'a str>, value: &'a str) {
        for word in WORD_REGEX.find_iter(value) {
            accumulator.push(word.as_str());
        }
    }

    fn collect_words_in_nodes<'a>(accumulator: &mut Vec<&'a str>, nodes: &[Node<'a>]) {
        for node in nodes {
            match node {
                Text { value, .. }          => collect_words_in_text(accumulator, value),
                ExternalLink { nodes, .. }  => collect_words_in_nodes(accumulator, nodes),
                Image { text, .. }          => collect_words_in_nodes(accumulator, text),
                Link { text, .. }           => collect_words_in_nodes(accumulator, text),

                _ => (),
            }
        }
    }

    let mut words = vec![];

    let template_open_count     = wikitext.matches(TEMPLATE_OPEN_TOKEN).count();
    let template_close_count    = wikitext.matches(TEMPLATE_CLOSE_TOKEN).count();

    // Workaround catastrophic backtracking and memory usage on malformed documents.
    // See https://github.com/portstrom/parse_wiki_text/issues/11 for details.
    if template_open_count <= template_close_count {
        let document = WIKITEXT_CONFIGURATION.parse(wikitext);

        collect_words_in_nodes(&mut words, &document.nodes);
    } else {
        collect_words_in_text(&mut words, wikitext);
    }

    words
}
